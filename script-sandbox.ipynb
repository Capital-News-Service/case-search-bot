{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load needed libraries\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1\n",
      "Scraping Page 2\n",
      "Scraping Page 3\n",
      "Scraping Page 4\n",
      "Scraping Page 5\n",
      "Scraping Page 6\n",
      "Done Scraping\n",
      "65\n",
      "17 New Cases\n",
      "send alert\n",
      "2-0254\n",
      "send alert\n",
      "1-1111\n",
      "1-1111\n",
      "send alert\n",
      "1-1111\n",
      "1-1111\n",
      "send alert\n",
      "1-1111\n",
      "send alert\n",
      "1-1137\n",
      "1-1137\n",
      "1-0521\n",
      "1-1161\n",
      "send alert\n",
      "1-1415\n",
      "1-1440\n",
      "send alert\n",
      "2-3040\n",
      "2-0254\n",
      "send alert\n",
      "2-3040\n",
      "2-0254\n",
      "send alert\n",
      "1-1502\n",
      "1-1167\n",
      "send alert\n",
      "1-1420\n",
      "1-1415\n",
      "1-1425\n",
      "2-0050\n",
      "send alert\n",
      "1-1464\n",
      "1-1465\n",
      "send alert\n",
      "1-1415\n",
      "send alert\n",
      "1-1415\n",
      "1-3605\n",
      "send alert\n",
      "1-1415\n",
      "send alert\n",
      "1-1415\n",
      "send alert\n",
      "1-1111\n",
      "send alert\n",
      "1-1420\n",
      "1-1415\n",
      "1-1425\n",
      "1-5202\n",
      "1-5200\n"
     ]
    }
   ],
   "source": [
    "#load in sensitive information from seperate key file\n",
    "keys={}\n",
    "with open(\"keys.json\",\"r\") as f:\n",
    "    keys = json.loads(f.read())\n",
    "    \n",
    "slack_url = keys[\"url\"]\n",
    "\n",
    "#Get an authenticated cookie for searches\n",
    "def getCookie():\n",
    "    session = requests.Session()\n",
    "    url = 'http://casesearch.courts.state.md.us/casesearch/'\n",
    "    params = {\n",
    "            'disclaimer' : 'Y',\n",
    "            'action' : 'Continue'\n",
    "    }\n",
    "    r = session.post(url, data=params)\n",
    "\n",
    "    cook = session.cookies['JSESSIONID']\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    return \"JSESSIONID=\" + cook\n",
    "    \n",
    "#search case search for single cases results by case number\n",
    "def getSingleCase(cookie, caseId):\n",
    "\n",
    "    headers = {'Cookie': cookie}\n",
    "\n",
    "    params = {\n",
    "        'caseId' : caseId,\n",
    "        'action': 'Get Case',\n",
    "        'locationCode': 'B'\n",
    "    }\n",
    "\n",
    "    url = 'http://casesearch.courts.state.md.us/casesearch/inquiryByCaseNum.jis'\n",
    "    r = requests.post(url, params=params, headers=headers)\n",
    "    return r.text\n",
    "\n",
    "#search casesearch for possible cases\n",
    "def getPage(cookie, page):\n",
    "    headers = {'Cookie': cookie}\n",
    "\n",
    "    params = {\n",
    "        'd-16544-p': page,\n",
    "        'lastName': '%', \n",
    "        'firstName' : '',\n",
    "        'middleName': '',  \n",
    "        'partyType': 'DEF',\n",
    "        'site': 'CRIMINAL',\n",
    "        'courtSystem': 'B',\n",
    "        'countyName': 'ANNE ARUNDEL COUNTY',\n",
    "        'filingStart': '1/14/2019',\n",
    "        'filingEnd': '1/15/2019',\n",
    "        'filingDate': '',\n",
    "        'company': 'N',\n",
    "        'action': 'Search',\n",
    "    }\n",
    "\n",
    "    url = 'http://casesearch.courts.state.md.us/casesearch/inquirySearch.jis'\n",
    "    r = requests.post(url, params=params, headers=headers)\n",
    "    time.sleep(1)\n",
    "    return r.text\n",
    "\n",
    "#Get charges for one individual cases\n",
    "def getCharges(cookie, caseId):\n",
    "    \n",
    "    #data we will gather from individual case page\n",
    "    charges = []\n",
    "    cjiss = []\n",
    "    text = getSingleCase(cookie, caseId)\n",
    "    soup = BeautifulSoup(text)\n",
    "    windows = soup.find_all(\"div\", attrs={'class':'AltBodyWindow1'})\n",
    "    for window in windows:\n",
    "        tables = window.find_all(\"table\")\n",
    "        for table in tables:\n",
    "            for row in table.findAll('tr'):\n",
    "                cell = row.findNext('td')\n",
    "                \n",
    "                #get cjis number for each charge\n",
    "                if cell.text == 'Charge No:':\n",
    "                    target = cell.next_sibling\n",
    "                    spans = target.find_all(\"span\")\n",
    "                    cjis = spans[2].text\n",
    "                    cjiss.append(cjis)\n",
    "\n",
    "                #get charge description for each charge\n",
    "                if cell.text == 'Charge Description:':\n",
    "                    target = cell.next_sibling\n",
    "                    spans = target.find_all(\"span\")\n",
    "                    charge = spans[0].text              \n",
    "                    charges.append(charge)\n",
    "     \n",
    "    charge_data = {\"charge\": charges, \"cjis\" : cjiss}\n",
    "    return charge_data\n",
    "    \n",
    "#Run search and return information on all current cases  \n",
    "def getCases(cookie):\n",
    "    \n",
    "    cases_on_page = 25\n",
    "    page = 1\n",
    "    \n",
    "    #data we will collect\n",
    "    caseIds = []\n",
    "    links = []\n",
    "    names = []\n",
    "    types = []\n",
    "    dates = []\n",
    "    \n",
    "    #keep scraping until you have reached the last page of results\n",
    "    while (cases_on_page == 25):\n",
    "        \n",
    "        text = getPage(cookie, page)\n",
    "        soup = BeautifulSoup(text)\n",
    "        table = soup.find(\"table\", attrs={'id':'row'})\n",
    "        body = table.find(\"tbody\")\n",
    "        rows = body.find_all(\"tr\")\n",
    "        cases_on_page = len(rows)\n",
    "\n",
    "        for row in rows:\n",
    "            tds = row.find_all(\"td\")\n",
    "            caseType = tds[5].text\n",
    "            if caseType == \"CRSCA\" or caseType == \"CROVA\":\n",
    "                links.append(\"http://casesearch.courts.state.md.us/casesearch/\" + tds[0].find(\"a\")['href'])\n",
    "                caseId = tds[0].find(\"a\").text\n",
    "                caseIds.append(caseId)\n",
    "                names.append(tds[1].text)\n",
    "                types.append(caseType)\n",
    "                dates.append(tds[7].text)\n",
    "\n",
    "        #create dataframe from gathered info\n",
    "        cases = pd.DataFrame(\n",
    "            {'caseId': caseIds,\n",
    "             'name': names,\n",
    "             \"type\": types,\n",
    "             \"date\": dates,\n",
    "             \"link\" : links\n",
    "            })\n",
    "        \n",
    "        print(\"Scraping Page \" + str(page))\n",
    "        page = page+1\n",
    "\n",
    "    print(\"Done Scraping\")\n",
    "    return cases\n",
    "\n",
    "#Post message on slack\n",
    "def send_alert(row, cookie):\n",
    "    print(\"send alert\")\n",
    "    charges = \"\"\n",
    "    charge_data = getCharges(cookie, row[\"caseId\"])\n",
    "    charge_num = 1\n",
    "    for c,j in zip(charge_data['charge'],charge_data['cjis']):\n",
    "        if charges == \"\":\n",
    "            charges = \"\\n1) \" + c + \" : \" + j\n",
    "        else:\n",
    "            charges = charges + \" \\n\" + str(charge_num) + \") \" + c + \" : \" + j\n",
    "        charge_num = charge_num + 1\n",
    "        \n",
    "    message = row['name'] + \" - \" + row['date'] + charges + \" \\n\" + row['link'] +\" \\n-------------------------\"\n",
    "    slack_data = {'text': message}\n",
    "    headers={'Content-Type': 'application/json'}\n",
    "    url = slack_url\n",
    "    r = requests.post(url, json=slack_data, headers=headers)\n",
    "\n",
    "#Find new cases and post them on slack\n",
    "def compare_cases(new_cases, cookie):\n",
    "    \n",
    "    #load cases from last search\n",
    "    old_cases = pd.read_json('cases.json')\n",
    "    \n",
    "    print(str(len(new_cases)-len(old_cases)) + \" New Cases\")\n",
    "\n",
    "    #see if any results are new and if they are post them on slack\n",
    "    for index, row in new_cases.iterrows():\n",
    "          if row[\"caseId\"] not in old_cases['caseId'].unique():\n",
    "            send_alert(row, cookie)\n",
    "\n",
    "#     new_cases.to_json('cases.json')\n",
    "  \n",
    "#Run bot\n",
    "def runBot():\n",
    "    cookie = getCookie()\n",
    "    cases = getCases(cookie)\n",
    "    compare_cases(cases, cookie)\n",
    "\n",
    "runBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
