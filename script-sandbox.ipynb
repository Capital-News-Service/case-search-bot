{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jagluck\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file c:\\users\\jagluck\\appdata\\local\\programs\\python\\python36-32\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1\n",
      "Scraping Page 2\n",
      "Scraping Page 3\n",
      "Scraping Page 4\n",
      "Scraping Page 5\n",
      "Scraping Page 6\n",
      "Done Scraping\n",
      "65\n",
      "0 New Cases\n"
     ]
    }
   ],
   "source": [
    "#load in sensitive information\n",
    "keys={}\n",
    "with open(\"keys.json\",\"r\") as f:\n",
    "    keys = json.loads(f.read())\n",
    "    \n",
    "slack_url = keys[\"url\"]\n",
    "\n",
    "#Get an authenticated cookie for searches\n",
    "def getCookie():\n",
    "    session = requests.Session()\n",
    "    url = 'http://casesearch.courts.state.md.us/casesearch/'\n",
    "    params = {\n",
    "            'disclaimer' : 'Y',\n",
    "            'action' : 'Continue'\n",
    "    }\n",
    "    r = session.post(url, data=params)\n",
    "\n",
    "    cook = session.cookies['JSESSIONID']\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    return \"JSESSIONID=\" + cook\n",
    "    \n",
    "def getSingleCase(cookie, caseId):\n",
    "\n",
    "    headers = {'Cookie': cookie}\n",
    "\n",
    "    params = {\n",
    "        'caseId' : caseId,\n",
    "        'action': 'Get Case',\n",
    "        'locationCode': 'B'\n",
    "    }\n",
    "\n",
    "    url = 'http://casesearch.courts.state.md.us/casesearch/inquiryByCaseNum.jis'\n",
    "    r = requests.post(url, params=params, headers=headers)\n",
    "    return r.text\n",
    "\n",
    "#Run search for results\n",
    "def getPage(cookie, page):\n",
    "    headers = {'Cookie': cookie}\n",
    "\n",
    "    params = {\n",
    "        'd-16544-p': page,\n",
    "        'lastName': '%', \n",
    "        'firstName' : '',\n",
    "        'middleName': '',  \n",
    "        'partyType': 'DEF',\n",
    "        'site': 'CRIMINAL',\n",
    "        'courtSystem': 'B',\n",
    "        'countyName': 'ANNE ARUNDEL COUNTY',\n",
    "        'filingStart': '1/14/2019',\n",
    "        'filingEnd': '1/15/2019',\n",
    "        'filingDate': '',\n",
    "        'company': 'N',\n",
    "        'action': 'Search',\n",
    "    }\n",
    "\n",
    "    url = 'http://casesearch.courts.state.md.us/casesearch/inquirySearch.jis'\n",
    "    r = requests.post(url, params=params, headers=headers)\n",
    "    time.sleep(1)\n",
    "    return r.text\n",
    "\n",
    "#Get charges for one individual cases\n",
    "def getCharges(cookie, caseId):\n",
    "    charges = []\n",
    "    text = getSingleCase(cookie, caseId)\n",
    "    soup = BeautifulSoup(text)\n",
    "    windows = soup.find_all(\"div\", attrs={'class':'AltBodyWindow1'})\n",
    "    for window in windows:\n",
    "        tables = window.find_all(\"table\")\n",
    "        for table in tables:\n",
    "            for row in table.findAll('tr'):\n",
    "                cell = row.findNext('td')\n",
    "                if cell.text == 'Charge Description:':\n",
    "                    target = cell.next_sibling\n",
    "                    spans = target.find_all(\"span\")\n",
    "                    charge = spans[0].text\n",
    "                            \n",
    "                    charges.append(charge)\n",
    "                if cell.text == 'Charge No:':\n",
    "                    target = cell.next_sibling\n",
    "                    spans = target.find_all(\"span\")\n",
    "                    if spans[1].text != \"CJIS Code:\":\n",
    "                        charge = spans[2].text\n",
    "                        charges.append(charge)\n",
    "\n",
    "    return charges\n",
    "    \n",
    "#Run search and return information on all current cases  \n",
    "def getCases(cookie):\n",
    "    \n",
    "    cases_on_page = 25\n",
    "    page = 1\n",
    "    \n",
    "    caseIds = []\n",
    "    links = []\n",
    "    names = []\n",
    "    types = []\n",
    "    dates = []\n",
    "    while (cases_on_page == 25):\n",
    "        \n",
    "        text = getPage(cookie, page)\n",
    "        soup = BeautifulSoup(text)\n",
    "        table = soup.find(\"table\", attrs={'id':'row'})\n",
    "        body = table.find(\"tbody\")\n",
    "        rows = body.find_all(\"tr\")\n",
    "        cases_on_page = len(rows)\n",
    "\n",
    "        for row in rows:\n",
    "            tds = row.find_all(\"td\")\n",
    "            caseType = tds[5].text\n",
    "            if caseType == \"CRSCA\" or caseType == \"CROVA\":\n",
    "                links.append(\"http://casesearch.courts.state.md.us/casesearch/\" + tds[0].find(\"a\")['href'])\n",
    "                caseId = tds[0].find(\"a\").text\n",
    "                caseIds.append(caseId)\n",
    "                names.append(tds[1].text)\n",
    "                types.append(caseType)\n",
    "                dates.append(tds[7].text)\n",
    "\n",
    "        cases = pd.DataFrame(\n",
    "            {'caseId': caseIds,\n",
    "             'name': names,\n",
    "             \"type\": types,\n",
    "             \"date\": dates,\n",
    "             \"link\" : links\n",
    "            })\n",
    "        \n",
    "        print(\"Scraping Page \" + str(page))\n",
    "        page = page+1\n",
    "\n",
    "    print(\"Done Scraping\")\n",
    "    return cases\n",
    "\n",
    "#Post message on slack\n",
    "def send_alert(row):\n",
    "    print(\"send alert\")\n",
    "    charges = \"\"\n",
    "    all_charges = getCharges(cookie, row[\"caseId\"])\n",
    "    charge_num = 1\n",
    "    for c in all_charges:\n",
    "        if charges == \"\":\n",
    "            charges = \"\\n1) \" + c\n",
    "        else:\n",
    "            charges = charges + \" \\n\" + str(charge_num) + \") \" + c\n",
    "        charge_num = charge_num + 1\n",
    "        \n",
    "    message= row['name'] + \" - \" + row['date'] + charges + \" \\n\" + row['link'] +\" \\ n-------------------------\"\n",
    "    slack_data = {'text': message}\n",
    "    headers={'Content-Type': 'application/json'}\n",
    "    url = slack_url\n",
    "    r = requests.post(url, json=slack_data, headers=headers)\n",
    "\n",
    "#Find new cases and post them on slack\n",
    "def compare_cases(new_cases):\n",
    "    print(len(new_cases))\n",
    "    old_cases = pd.read_json('cases.json')\n",
    "    \n",
    "    print(str(len(new_cases)-len(old_cases)) + \" New Cases\")\n",
    "\n",
    "    for index, row in new_cases.iterrows():\n",
    "\n",
    "        if row[\"caseId\"] not in old_cases['caseId'].unique():\n",
    "            send_alert(row)\n",
    "\n",
    "#     new_cases.to_json('cases.json')\n",
    "  \n",
    "#Run bot\n",
    "def runBot():\n",
    "    cookie = getCookie()\n",
    "    cases = getCases(cookie)\n",
    "    compare_cases(cases)\n",
    "\n",
    "runBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
